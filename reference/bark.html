<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="BARK is a Bayesian sum-of-kernels model.
For numeric response \(y\), we have
\(y = f(x) + \epsilon\),
where \(\epsilon \sim N(0,\sigma^2)\).
For a binary response \(y\), \(P(Y=1 | x) = F(f(x))\),
where \(F\)
denotes the standard normal cdf (probit link).

In both cases, \(f\) is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function \(f\).
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function \(f\).
Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
e, d, enabling
either soft shrinkage or  se, sd, enabling hard shrinkage for the scale
parameters."><title>Nonparametric Regression using Bayesian Additive Regression Kernels — bark • bark</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Nonparametric Regression using Bayesian Additive Regression Kernels — bark"><meta property="og:description" content="BARK is a Bayesian sum-of-kernels model.
For numeric response \(y\), we have
\(y = f(x) + \epsilon\),
where \(\epsilon \sim N(0,\sigma^2)\).
For a binary response \(y\), \(P(Y=1 | x) = F(f(x))\),
where \(F\)
denotes the standard normal cdf (probit link).

In both cases, \(f\) is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function \(f\).
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function \(f\).
Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
e, d, enabling
either soft shrinkage or  se, sd, enabling hard shrinkage for the scale
parameters."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">bark</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../articles/index.html">Vignettes</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Functions</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://twitter.com/merliseclyde">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/merliseclyde/BAS">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Nonparametric Regression using Bayesian Additive Regression Kernels</h1>
      <small class="dont-index">Source: <a href="https://github.com/merliseclyde/bark/blob/HEAD/R/bark.r" class="external-link"><code>R/bark.r</code></a></small>
      <div class="d-none name"><code>bark.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>BARK is a Bayesian <em>sum-of-kernels</em> model.<br>
For numeric response \(y\), we have
\(y = f(x) + \epsilon\),
where \(\epsilon \sim N(0,\sigma^2)\).<br>
For a binary response \(y\), \(P(Y=1 | x) = F(f(x))\),
where \(F\)
denotes the standard normal cdf (probit link).
<br>
In both cases, \(f\) is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function \(f\).
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function \(f\).</p>
<p>Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
<em>e</em>, <em>d</em>, enabling
either soft shrinkage or  <em>se</em>, <em>sd</em>, enabling hard shrinkage for the scale
parameters.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">bark</span><span class="op">(</span></span>
<span>  <span class="va">formula</span>,</span>
<span>  <span class="va">data</span>,</span>
<span>  <span class="va">subset</span>,</span>
<span>  na.action <span class="op">=</span> <span class="va">na.omit</span>,</span>
<span>  testdata <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  selection <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  common_lambdas <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  classification <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keepevery <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  nburn <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  nkeep <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  printevery <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  keeptrain <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  fixed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  tune <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lstep <span class="op">=</span> <span class="fl">0.5</span>, frequL <span class="op">=</span> <span class="fl">0.2</span>, dpow <span class="op">=</span> <span class="fl">1</span>, upow <span class="op">=</span> <span class="fl">0</span>, varphistep <span class="op">=</span> <span class="fl">0.5</span>, phistep <span class="op">=</span></span>
<span>    <span class="fl">1</span><span class="op">)</span>,</span>
<span>  theta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>formula</dt>
<dd><p>model formula for the model with all predictors,
Y ~ X.  THe X variables will be centered and scaled as part of model fitting.</p></dd>


<dt>data</dt>
<dd><p>a data frame.  Factors will be converted to numerical vectors based on
the using `model.matrix`.</p></dd>


<dt>subset</dt>
<dd><p>an optional vector specifying a subset of observations to be
used in the fitting process.</p></dd>


<dt>na.action</dt>
<dd><p>a function which indicates what should happen when the data
contain NAs. The default is "na.omit".</p></dd>


<dt>testdata</dt>
<dd><p>Dataframe with test data for out of sample prediction.<br>
Should have same structure as data.</p></dd>


<dt>selection</dt>
<dd><p>Logical variable indicating whether variable 
dependent kernel parameters \(\lambda\) may be set to zero in the MCMC; 
default is TRUE. <br></p></dd>


<dt>common_lambdas</dt>
<dd><p>Logical variable indicating whether
kernel parameters \(\lambda\) should be predictor specific or common across
predictors;  default is TRUE.   Note if  <em>common_lambdas = TRUE</em> and 
<em>selection = TRUE</em> this applies just to the non-zero \(lambda_j\). <br></p></dd>


<dt>classification</dt>
<dd><p>TRUE/FALSE logical variable,
indicating a classification or regression problem.</p></dd>


<dt>keepevery</dt>
<dd><p>Every keepevery draw is kept to be returned to the user</p></dd>


<dt>nburn</dt>
<dd><p>Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.</p></dd>


<dt>nkeep</dt>
<dd><p>Number of MCMC iterations kept for the posterior inference.<br>
nkeep*keepevery iterations after the burn in.</p></dd>


<dt>printevery</dt>
<dd><p>As the MCMC runs, a message is printed every printevery draws.</p></dd>


<dt>keeptrain</dt>
<dd><p>Logical, whether to keep results for training samples.</p></dd>


<dt>verbose</dt>
<dd><p>Logical, whether to print out messages</p></dd>


<dt>fixed</dt>
<dd><p>A list of fixed hyperparameters, using the default values if not
specified.<br>
alpha = 1: stable index, must be 1 currently.<br>
eps = 0.5: approximation parameter.<br>
gam = 5: intensity parameter.<br>
la = 1: first argument of the gamma prior on kernel scales.<br>
lb = 2: second argument of the gamma prior on kernel scales.<br>
pbetaa = 1: first argument of the beta prior on plambda.<br>
pbetab = 1: second argument of the beta prior on plambda.<br>
n: number of training samples, automatically generates.<br>
p: number of explanatory variables, automatically generates.<br>
meanJ: the expected number of kernels, automatically generates.</p></dd>


<dt>tune</dt>
<dd><p>A list of tuning parameters, not expected to change.<br>
lstep: the stepsize of the lognormal random walk on lambda.<br>
frequL: the frequency to update L.<br>
dpow: the power on the death step.<br>
upow: the power on the update step.<br>
varphistep: the stepsize of the lognormal random walk on varphi.<br>
phistep: the stepsize of the lognormal random walk on phi.</p></dd>


<dt>theta</dt>
<dd><p>A list of the starting values for the parameter theta,
use defaults if nothing is given.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p><code>bark</code> returns a list, including:</p>
<dl><dt>call</dt>
<dd><p>the matched call</p></dd>

 <dt>fixed</dt>
<dd><p>Fixed hyperparameters</p></dd>

 <dt>tune</dt>
<dd><p>Tuning parameters used</p></dd>

 <dt>theta.last</dt>
<dd><p>The last set of parameters from the posterior draw</p></dd>

 <dt>theta.nvec</dt>
<dd><p>A matrix with nrow(x.train)\(+1\) rows and (nkeep) columns,
recording the  number of kernels at each training sample</p></dd>

 <dt>theta.varphi</dt>
<dd><p>A matrix with nrow(x.train)
 \(+1\) rows and (nkeep) columns,
 recording the precision in the normal gamma prior
 distribution for the regression coefficients</p></dd>

 <dt>theta.beta</dt>
<dd><p>A matrix with nrow(x.train)\(+1\) rows and (nkeep) columns,
 recording the regression coefficients</p></dd>

 <dt>theta.lambda</dt>
<dd><p>A matrix with ncol(x.train) rows and (nkeep) columns,
  recording the kernel scale parameters</p></dd>

 <dt>thea.phi</dt>
<dd><p>The vector of length nkeep,
 recording the precision in regression Gaussian noise
 (1 for the classification case)</p></dd>

 <dt>yhat.train</dt>
<dd><p>A matrix with nrow(x.train) rows and (nkeep) columns.
 Each column corresponds to a draw \(f^*\) from
 the posterior of \(f\)
  and each row corresponds to a row of x.train.
 The \((i,j)\) value is \(f^*(x)\) for
 the \(j^{th}\) kept draw of \(f\)
 and the \(i^{th}\) row of x.train.<br>
 For classification problems, this is the value
 of the expectation for the underlying normal
 random variable.<br>
 Burn-in is dropped</p></dd>

<dt>yhat.test</dt>
<dd><p>Same as yhat.train but now the x's
are the rows of the test data</p></dd>

<dt>yhat.train.mean</dt>
<dd><p>train data fits = row mean of yhat.train</p></dd>

<dt>yhat.test.mean</dt>
<dd><p>test data fits = row mean of yhat.test</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters etc.</p>
<p>Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
\(f^*(x)\) (and \(\sigma^*\) in the numeric case)
where * denotes a particular draw.
The \(x\) is either a row from the training data (x.train)</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>Other bark functions: 
<code><a href="bark-package-deprecated.html">bark-package-deprecated</a></code>,
<code><a href="bark-package.html">bark-package</a></code>,
<code><a href="sim_Friedman1.html">sim_Friedman1</a>()</code>,
<code><a href="sim_Friedman2.html">sim_Friedman2</a>()</code>,
<code><a href="sim_Friedman3.html">sim_Friedman3</a>()</code>,
<code><a href="sim_circle.html">sim_circle</a>()</code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co">##Simulated regression example</span></span></span>
<span class="r-in"><span><span class="co"># Friedman 2 data set, 200 noisy training, 1000 noise free testing</span></span></span>
<span class="r-in"><span><span class="co"># Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)</span></span></span>
<span class="r-in"><span><span class="co"># Out of sample MSE in BART (default):    5300 (sd. 1000)</span></span></span>
<span class="r-in"><span><span class="va">traindata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="fu"><a href="sim_Friedman2.html">sim_Friedman2</a></span><span class="op">(</span><span class="fl">200</span>, sd<span class="op">=</span><span class="fl">125</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">testdata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="fu"><a href="sim_Friedman2.html">sim_Friedman2</a></span><span class="op">(</span><span class="fl">1000</span>, sd<span class="op">=</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">fit.bark.d</span> <span class="op">&lt;-</span> <span class="fu">bark</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">traindata</span>, testdata<span class="op">=</span> <span class="va">testdata</span>,</span></span>
<span class="r-in"><span>                   nburn<span class="op">=</span><span class="fl">10</span>, nkeep<span class="op">=</span><span class="fl">100</span>, keepevery<span class="op">=</span><span class="fl">10</span>,</span></span>
<span class="r-in"><span>                   classification<span class="op">=</span><span class="cn">FALSE</span>, </span></span>
<span class="r-in"><span>                   common_lambdas <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>                   selection <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html" class="external-link">boxplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">fit.bark.d</span><span class="op">$</span><span class="va">theta.lambda</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-plt img"><img src="bark-1.png" alt="" width="700" height="433"></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">fit.bark.d</span><span class="op">$</span><span class="va">yhat.test.mean</span><span class="op">-</span><span class="va">testdata</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] 2601.994</span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span> <span class="co">##Simulate classification example</span></span></span>
<span class="r-in"><span> <span class="co"># Circle 5 with 2 signals and three noisy dimensions</span></span></span>
<span class="r-in"><span> <span class="co"># Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)</span></span></span>
<span class="r-in"><span> <span class="co"># Out of sample error rate in BART (default):    0.065 (sd. 0.02)</span></span></span>
<span class="r-in"><span> <span class="va">traindata</span> <span class="op">&lt;-</span> <span class="fu">sim_Circle</span><span class="op">(</span><span class="fl">200</span>, dim<span class="op">=</span><span class="fl">5</span><span class="op">)</span></span></span>
<span class="r-in"><span> <span class="va">testdata</span> <span class="op">&lt;-</span> <span class="fu">sim_Circle</span><span class="op">(</span><span class="fl">1000</span>, dim<span class="op">=</span><span class="fl">5</span><span class="op">)</span></span></span>
<span class="r-in"><span> <span class="va">fit.bark.se</span> <span class="op">&lt;-</span> <span class="fu">bark</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, </span></span>
<span class="r-in"><span>                     data<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">traindata</span><span class="op">)</span>, </span></span>
<span class="r-in"><span>                     testdata<span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">testdata</span><span class="op">)</span>, </span></span>
<span class="r-in"><span>                     classification<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></span>
<span class="r-in"><span> <span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html" class="external-link">boxplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">fit.bark.se</span><span class="op">$</span><span class="va">theta.lambda</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">fit.bark.se</span><span class="op">$</span><span class="va">yhat.test.mean</span><span class="op">&gt;</span><span class="fl">0</span><span class="op">)</span><span class="op">!=</span><span class="va">testdata</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by <a href="http://stat.duke.edu/~clyde" class="external-link">Merlise Clyde</a>, Zhi Ouyang.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

